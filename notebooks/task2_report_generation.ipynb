{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aa63386-9bb9-4d74-bb4e-fae261bce98d",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd87bb2f-8664-487d-a8a2-3388eb2a689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate bitsandbytes pillow pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eb97c2-5e91-4ead-a0d5-905ddeada841",
   "metadata": {},
   "source": [
    "# Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e462aa-9fe9-4b6b-9701-8e0b7eb98018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "IMAGE_FOLDER = \"/content/drive/MyDrive/images\"\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9063fa-e912-476d-8162-00f74e4c4d12",
   "metadata": {},
   "source": [
    "# Imports + Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639e3699-7b71-4b6d-8ac3-dabca67a3fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "# =============================\n",
    "# CONFIG\n",
    "# =============================\n",
    "\n",
    "MODEL_ID = \"google/medgemma-1.5-4b-it\"\n",
    "\n",
    "IMAGE_FOLDER = \"/content/images\"        # Change if needed\n",
    "REPORT_FOLDER = \"/content/reports\"\n",
    "CSV_OUTPUT = \"/content/vlm_results.csv\"\n",
    "\n",
    "MAX_IMAGES = 10\n",
    "MAX_NEW_TOKENS = 80\n",
    "\n",
    "os.makedirs(REPORT_FOLDER, exist_ok=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e042817-7524-4812-803a-ff104de7a8ec",
   "metadata": {},
   "source": [
    "# 4-bit Quantization Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c79efb-ec43-499b-9759-befa2b827ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f110d0ee-5597-4814-9781-ba2d157725c1",
   "metadata": {},
   "source": [
    "# Load MedGemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40644915-01b9-41a7-89ab-342b9cddba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading MedGemma (4bit)...\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    use_fast=False\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad71c45-d50d-4910-8e02-2fbc533998e4",
   "metadata": {},
   "source": [
    "# Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57c0b6e-9e29-4088-90d8-4edf494923b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = [\n",
    "    os.path.join(IMAGE_FOLDER, f)\n",
    "    for f in os.listdir(IMAGE_FOLDER)\n",
    "    if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "][:MAX_IMAGES]\n",
    "\n",
    "print(f\"Found {len(image_paths)} images.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441dfb41-7022-4203-9ce0-83a046e29fc2",
   "metadata": {},
   "source": [
    "# Prompt Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92490a11-d940-4946-bfc8-29767241acb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_messages():\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (\n",
    "                        \"You are an expert radiologist.\\n\"\n",
    "                        \"Analyze this chest X-ray.\\n\\n\"\n",
    "                        \"Return:\\n\\n\"\n",
    "                        \"Findings:\\n\"\n",
    "                        \"- ...\\n\\n\"\n",
    "                        \"Abnormalities:\\n\"\n",
    "                        \"- ...\\n\\n\"\n",
    "                        \"Impression:\\n\"\n",
    "                        \"- Pneumonia likely or unlikely\"\n",
    "                    ),\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5027f1-eaf5-46cf-9b3b-25fb7e7682ff",
   "metadata": {},
   "source": [
    "# Generate Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffb2aa5-fd69-4404-8085-d0c059135fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for i, img_path in enumerate(image_paths):\n",
    "\n",
    "    print(f\"\\n[{i+1}/{len(image_paths)}] Processing {os.path.basename(img_path)}\")\n",
    "\n",
    "    try:\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = image.resize((512, 512))\n",
    "\n",
    "        messages = build_messages()\n",
    "\n",
    "        prompt = processor.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        inputs = processor(\n",
    "            text=prompt,\n",
    "            images=[image],\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Move inputs to same device as model\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                do_sample=False,\n",
    "                temperature=0.0\n",
    "            )\n",
    "\n",
    "        generated_tokens = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "\n",
    "        report = processor.decode(\n",
    "            generated_tokens,\n",
    "            skip_special_tokens=True\n",
    "        ).strip()\n",
    "\n",
    "        if len(report) < 10:\n",
    "            report = \"No clear findings generated.\"\n",
    "\n",
    "        report_file = os.path.join(\n",
    "            REPORT_FOLDER,\n",
    "            os.path.basename(img_path) + \".txt\"\n",
    "        )\n",
    "\n",
    "        with open(report_file, \"w\") as f:\n",
    "            f.write(report)\n",
    "\n",
    "        results.append({\n",
    "            \"image\": img_path,\n",
    "            \"report\": report\n",
    "        })\n",
    "\n",
    "        print(\"✔ Done\")\n",
    "\n",
    "        del inputs, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"❌ Error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53545f90-c638-4b0d-b5eb-dd7049bf2263",
   "metadata": {},
   "source": [
    "# Save CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7630653d-913c-4494-b203-bdbff1def1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(CSV_OUTPUT, index=False)\n",
    "    print(\"CSV saved at:\", CSV_OUTPUT)\n",
    "\n",
    "print(\"\\nAll reports generated.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
